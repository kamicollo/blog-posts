---
title: "Statistical guarantees and long-term error rates in A/B testing"
subtitle: "Why considering just Type I and II error rates is not sufficient"
format: html
editor: visual
---

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
library(rbenchmark)
library(tidyverse)
library(ggplot2)
library(kableExtra)
library(pbapply)
```


Suppose you're the *Head of Something Data Related* in a company that runs many experiments annually, and your job is to ensure that the experimentation program is trustworthy. It's still in the early stages of the experimentation journey in this company, so most tests use null hypothesis testing (NHT) with fixed sample sizes (although you're looking into sequential testing and CUPED, too). You set the following rules for every experiment:

-   Only experiments with a p-value \< 0.05 will be considered as having conclusive outcomes (you target a Type I error rate $\alpha=0.05$).

-   Every experiment should be run with a predetermined sample size, which implies a power of 80% (you target a Type II error rate $\beta=0.1$).

Setting aside the considerations on whether these are the [best thresholds and other issues with using NHT](https://aurimas.eu/blog/2023/02/getting-faster-to-decisions-in-a-b-tests-part-2-misinterpretations-and-practical-challenges-of-classical-hypothesis-testing/), here's one question that, arguably, should matter to someone responsible for an experimentation program:

> What long-term error rates (a.k.a. bad decisions) can I expect if I run the experimentation program with such parameters?

It may seem the answer is simply Type I and Type II error rates, or, "We will detect (i.e., declare them as statistically significant) differences when they exist, 80% of the time, and, in 5% of the cases we detect differences, they will be due to imperfect sampling randomization and not true differences". But that is only true under a few conditions:

-   When estimating the required sample size in every experiment, you will always choose MDE precisely equal to the actual unobservable difference for that specific experiment.

-   You will have some experiments in which the actual difference is precisely zero—not "close enough to zero" but literally zero.

Neither of these conditions is likely to hold in the real world, which is why Type I and II error rates aren't likely to represent the long-term error rates. They have not been designed for that. Instead, they represent frequentist statistics and its definition of probability—the likelihood of observing the same results if you repeat the same experiment over and over.

As a *Head of Something Data,* however, you're not only interested in whether the results of a particular experiment hold. You (should!) want to know what the long-term error rates are.

For that, we need some other measures & some other tools.

## Moving beyond Type I error rates: Type M and S errors

Type I error rate, $\alpha$ is the most known statistical concept among everyone, including non-technical stakeholders. It's probably the most misinterpreted one, too, given all the issues people have with p-values.

For something so popular, I'd argue it's not so valuable because:

-   It's independent of the sample size. Isn't that weird? No matter how much data you collect, you're guaranteed that, if there's no underlying difference, 5% of the time, you will still find statistically significant differences in the data when using a p-value of 0.05. Intuitively, one would think that more data equals more confidence in results, but the Type I error rate doesn't change! Yes, the estimates become tighter, but Type I error rate doesn't measure that.

-   How often do you run experiments that have precisely no impact? Hopefully, never. So you'll never need to worry about it! It's tempting to approximate zero with "effect sizes so small they don't matter," but Type I error rates say nothing about small effect sizes.

[Gelman and Carlin](https://journals.sagepub.com/doi/10.1177/1745691614551642) (2014) introduced two other metrics that I think are much more useful to consider:

-   Type S (sign) error: the probability that an observed difference, when declared as statistically significant, has an opposite sign than the true difference. In other words, the probability that you will be making a decision that's opposite to what you are really after.
-   Type M (Magnitude) error: a measure of how much the observed difference, when declared as statistically significant, is different from true difference, expressed as a ratio ($\frac{\text{observed difference}}{\text{true difference}}$). It helps quantify the magnitude of winner's curse phenomenon, as well as partially addresses the issue with quantifying risk of declaring "effect sizes so small that they don't practically matter" as statistically significant.

Type M and S error rates directly translate to business implications, unlike Type I error. They become smaller as sample sizes increase, as they operate in the "when alternative hypothesis is true" land, i.e. in situations when the effect sizes are not exactly zero.

### Calculating Type S and M error rates

The original paper proposed calculating Type S and M error rates via simulation using the following function, where $A$ is the effect size, $s$ is the standard error, $\alpha$ is the chosen level of statistical significance, and $df$ represents the degrees of freedom.


```{r}
retrodesign <- function(A, s, alpha=.05, df=Inf, n.sims=10000){
  z <- qt(1-alpha/2, df)
  p.hi <- 1 - pt(z-A/s, df)
  p.lo <- pt(-z-A/s, df)
  power <- p.hi + p.lo
  typeS <- p.lo/power
  estimate <- A + s*rt(n.sims,df)
  significant <- abs(estimate) > s*z
  exaggeration <- mean(abs(estimate)[significant])/A
  return(list(power=power, typeS=typeS, typeM=exaggeration))
}
```


If we wanted to compute the metrics for a hypothetical A/B test with a binary outcome metric (e.g. conversion rate) where the baseline is *17%,* MDE of interest is *1ppt*, and sample size was 6000 users in each group:


```{r}
n_group = 6000
baseline = 0.17
delta = 0.01

#calculate the baseline variance
baseline_var = baseline * (1 - baseline)

#calculate standard error by estimating pooled variance
st_error = sqrt(baseline_var / n_group + baseline_var / n_group)

#degrees of freedom is N*2 - 2
df_test = n_group * 2 - 2

#use Gelman's function to get error rate estimates
gelman_formula = retrodesign(A=delta, s=st_error, df = df_test) |>
  as_tibble() |> mutate(method = 'Gelman/Carlin simulation')
  
gelman_formula |> kbl(caption = 'Estimated Metrics') |> kable_styling()
```


We can validate power calculations using R's built-in function - power estimates are very close (differences due to simulation error).


```{r}
power.prop.test(n=n_group, p1=baseline, p2 = baseline + delta)
```


In 2019, [Lu, Qui and Deng](https://pubmed.ncbi.nlm.nih.gov/29569719/) published a paper that includes closed form formulas for these error rates. The implementation of them is straightforward, too (and can be easily extended to unequal sample size designs):


```{r}

calculate_rates = function(delta, sd, n) {
  # delta - effect size (MDE)
  # sd - baseline standard deviation
  # n - size per group
  lambda = abs(delta) / sqrt(sd**2/n + sd**2/n)
  z = qnorm(1 - 0.05/2)

  neg = pnorm(-z -lambda)
  diff = pnorm(z - lambda)
  pos = pnorm(z + lambda)
  inv_diff = pnorm(lambda - z)

  
  list(
    power = neg + 1 - diff,
    typeS = neg / (neg + 1 - diff),
    typeM = (
      dnorm(lambda + z) + dnorm(lambda - z) + lambda * (pos + inv_diff - 1)
    ) / (lambda * (1 - pos + inv_diff))
  )
  
}
```


We can verify that we get the same results:


```{r}

closed_form = calculate_rates(delta=delta, sd=sqrt(baseline_var), n = n_group) |>
  as_tibble() |> mutate(method = 'Closed-form formula')

bind_rows(gelman_formula, closed_form) |> 
  kbl(caption = 'Estimated Metrics') |> kable_styling()
```


Finally, we can verify that all these formulas are correct with a small simulation where we repeat the same experiment with above parameters many times and estimate empirical error rate estimates.


```{r, message=FALSE, warning=F}

pboptions(type="none")
sim_results = pbsapply(1:10000, function(i) {
  set.seed(i*42)
  A = rbinom(n_group, 1, baseline)
  B = rbinom(n_group, 1, baseline + delta)
  mean_diff = mean(B) - mean(A)
  p_value = t.test(B, A)$p.value
  
  c(
    detected = p_value <= 0.05,
    sign_error = ifelse(p_value <= 0.05, sign(mean_diff) != sign(delta), NA),
    ratio = ifelse(p_value <= 0.05, mean_diff / delta, NA)
  )
  
}, cl=4) |> t() |> as_tibble()


empirical_rates = sim_results |> summarize(
  power = mean(detected, na.rm=T),
  typeS = mean(sign_error, na.rm=T),
  typeM = mean(ratio, na.rm=T),
  method = 'Empirical results'
)

bind_rows(gelman_formula, closed_form, empirical_rates) |> 
  kbl(caption = 'Estimated Metrics') |> kable_styling()

```


## Quantifying decision making risk with Type S and M error rates in a single experiment

Lu, Qiu and Deng paper includes a few charts that I replicate below as I find them very good illustrations of the risk associated with Type S and M error rates. On the x-axis, we plot statistical power, and on the y-axis - Type S error rate / Type M ratio:


```{r}

error_rates = lapply(seq(500, 40000, 500), function(n) {
  calculate_rates(delta, sqrt(baseline_var), n)
}) |> bind_rows() |> pivot_longer(-power)


ggplot(error_rates, aes(x=power)) + 
  geom_line(aes(y=value, color=name)) +
  scale_x_continuous(breaks=seq(0.05, 0.95, 0.1)) +
  facet_wrap(~name, scales='free_y') +
  theme_light() + 
  theme(
    panel.grid.major.x = element_blank(),
    legend.position = "none"
  )
```


We can see that Type S error (which is really bad, decision-making wise!) largely disappears once we hit power of $~0.35-0.4$. On the other hand, Type M ratio only goes below 1.5x once we achieve power of $0.5$. If this does not illustrate the issues with underpowered experiments, I don't know what does!

## Estimating long-term error rates

Type S and M error rates, while (imo!) much more useful than pure Type I and II errors, are still tightly relate to frequentist statistics concepts and are only valuable if we assume that we will guess the effect sizes perfectly in each experiment. If we could do that, we would not need to run experiments, of course.

To get to long-term error rates we need to replace this assumption with an assumption of a **distribution** of likely effect sizes. Instead of setting an MDE of $0.015$, we could, for example, assume that our effect sizes are normally distributed with a $\mu=0.015, \sigma=0.014$. Or, perhaps a bit more realistically, we may choose a non-symetric distribution and, keeping the same mean/standard distribution, assume that a lot of experiments have smaller effect sizes, but there's also a longer tail of positive effects, and no situations where experiments do *very badly* (because dogfooding, UXR and etc.).


```{r}
avg_treatment_effect = 0.015
sd_treatment_effect = 0.014


eff_size_generator_normal = function()  {
  rnorm(1, avg_treatment_effect, sd_treatment_effect)
}

eff_size_density_normal = function(x) {
  dnorm(x, mean = avg_treatment_effect, sd = sd_treatment_effect) 
}

eff_size_generator_gamma = function() {
  rgamma(1, 2, 2) / 50 - 1/50 + avg_treatment_effect
}

eff_size_density_gamma = function(x) {
  dgamma((x - avg_treatment_effect + 1/50) * 50, 2, 2) * 50
}

ggplot() + 
  stat_function(
    aes(color='Normally distributed effect sizes'), 
    fun = eff_size_density_normal
  ) + 
  stat_function(
    aes(color='Asymmetrically distributed effect sizes'), 
    fun = eff_size_density_gamma
  ) + 
  xlim(-0.05, 0.1) + labs(
    y='Density',
    color='Assumed effect size distribution',
    x='Effect size',
    title='Some possible effect size assumptions'
  )
```


If we were to simply use the average expected sample size, our error rates would be:


```{r}
calculate_error_rates(A=)
```


How do we calculate, say, power over a distribution of possible effect sizes? It's actually pretty simple - we integrate:


```{r, warning=FALSE, message=F}
integrate(function(x) {
  eff_size_density(x) * 
  replace_na(power.prop.test(n=n_group, p1=baseline, p2=baseline + x)$power, 0)
  
}, -Inf, Inf)
```


When you click the **Render** button a document will be generated that includes both content and the output of embedded code. You can embed code like this:


```{r}
n = 2000
mu = 0.01
sd = 0.2

power.t.test(n=n, delta=mu, sd=sd)
```

```{r}



calculate_rates(-mu, sd, n)



```

```{r}


retrodesign(mu, sqrt(sd**2/n + sd**2/n), df=n*2-2)
```

```{r}
benchmark(
  "my" = { calculate_rates(mu, sd, n)},
  "def" = { power.t.test(n=n, delta=mu, sd=sd)},
  "gel" = {retrodesign(mu, sqrt(sd**2/n + sd**2/n), df=n*2-2)},
  replications = 1000
)
```

```{r}

baseline = 0.17
n_group = 6000
sd_treatment_effect = 0.015
avg_treatment_effect = 0.003

eff_size_generator_normal = function()  {
  rnorm(1, avg_treatment_effect, sd_treatment_effect)
}

eff_size_density_normal = function(x) {
  dnorm(x, mean = avg_treatment_effect, sd = sd_treatment_effect) 
}

eff_size_generator_gamma = function() {
  rgamma(1, 2, 2) / 50 - 1/50 + avg_treatment_effect
}

eff_size_density_gamma = function(x) {
  dgamma((x - avg_treatment_effect + 1/50) * 50, 2, 2) * 50
}

eff_size_density = eff_size_density_gamma
eff_size_generator = eff_size_generator_gamma

power.prop.test(n=n_group, p1=baseline, p2 = baseline + avg_treatment_effect)

```

```{r}
ggplot() + stat_function(fun = eff_size_density) + xlim(-0.05, 0.05)
```


```{}
```


```{r, warning=FALSE, message=F}

baseline_sd = sqrt(baseline * (1 - baseline))

integrate(function(x) {
  r = calculate_rates(n=n_group, delta=x, sd=baseline_sd)
  eff_size_density(x) * (r$m_ratio - 1) * abs(x) * r$power

}, -Inf, Inf, stop.on.error = F)


integrate(function(x) {
  r = calculate_rates(n=n_group, delta=x, sd=baseline_sd)
  r$sign_error * r$power * eff_size_density(x)
  
}, -Inf, Inf, stop.on.error = F)

```

```{r}
ggplot() + stat_function(
  fun = function(x) { 
    r = retrodesign(
      A=x, s=sqrt(sd_baseline**2/n_group + sd_baseline**2/n_group), 
      df=n_group*2-2)
    abs(r$exaggeration)
}
) + xlim(-0.05, 0.05)
```

```{r}
retrodesign(0.00001, sqrt(sd**2/n + sd**2/n), df=n*2-2)
```

