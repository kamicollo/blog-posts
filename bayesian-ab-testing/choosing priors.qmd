---
title: "Choosing priors"
format: html
editor: visual
---

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
library(ggplot2)
library(tidyverse)
library(kableExtra)
library(scales)
library(pbapply)
```

```{r}
difference_of_two_normals = function(X, Y) {
  list (
    mean = X$mean - Y$mean,
    tau = 1 / ((1 / X$tau) + (1 / Y$tau)),
    sd = sqrt((1 / X$tau) + (1 / Y$tau))
  )
}

calculate_beta_posterior = function(X, prior){
  list(
    alpha = prior$alpha + sum(X),
    beta = prior$beta + (length(X) - sum(X))
  )
}

#calculate density at a given point X for the difference between two beta distributions
pdf_beta_diff = function(X, d1, d2) {
  
  sapply(X, function(t) {
    integrate(function(i) {
      (dbeta(i, d1$alpha, d1$beta) * dbeta(i-t, d2$alpha, d2$beta) + dbeta(i, d2$alpha, d2$beta) * dbeta(i+t, d1$alpha, d1$beta)) / 2
    }, lower=-Inf, upper=Inf, stop.on.error = F)$value
    })
  
}

#calculate a total CDF between X and Y for two beta distributions
cdf_beta_diff = function(X, Y, d1, d2) {
  integrate(pdf_beta_diff, d1, d2, lower=X, upper=Y, stop.on.error = F)$value
}
```

```{r}
eff = 0.02
baseline = 0.17
n_group = 6000

set.seed(42)
get_samples = function(n_group, baseline, eff) {
  list(
    A = rbinom(n_group, 1, baseline),
    B = rbinom(n_group, 1, baseline + eff)
  )
}
data = get_samples(n_group, baseline, eff)
```

So, how do we go about setting better priors?

### Choosing priors

Many discussions center around setting a prior for the treatment effect. Indeed, it can be hard to decide on that directly, similar to choosing an appropriate MDE. Setting the prior for the treatment effect also changes the modeling approach (nowhere above did we have to explicitly set it). It becomes necessary in more complex approaches (e.g., in the case of hierarchical models when running tests with multiple variants or CUPED-style regressions with extra covariates).

However, in a basic Bayesian A/B testing scenario, reasoning about priors on the data is simpler. We can always check what that implies for the treatment effect, anyway.

Let's continue with the conversion rate examples. The baseline conversion rate I used above was 17%. In a real-world setting, you would also know the baseline rate before starting the experiment. It feels like an excellent place to start.

Next, we need to convert it to a prior in terms of Beta distribution, which is parameterized by $\alpha$ and $\beta$, where $\alpha$ has an interpretation of "number of successes" and $\beta$ - number of failures. For one, we want the mean of the distribution,$\frac{\alpha}{\alpha + \beta}$, to be 0.17. How do we go about choosing the absolute values?

One approach (not the best, though!) is to think about them in relation to expected sample sizes. If we want the prior to be "as strong as the data," we would set $\alpha + \beta = n$. That's a bit extreme - what if we select it to be \~10% as strong as the data? 20%? 5%? We can use the same convolution approach as above and compute the distributions of the prior treatment effect implied by these choices.

```{r}
priors = c(
  #flat prior
  list(
    list(
      name = "Flat", 
      data = list(alpha=1, beta=1)
    )
  ),
  # priors of various strength
  lapply(
    c(0.05, 0.1, 0.2, 0.5, 1), 
    function(s) {
      list(
        name = paste(
          format(s * n_group, nsmall=0, width=5), 
          "-observations strong", sep=""
        ),
        data = list(
          alpha=baseline * n_group * s,
          beta=n_group * (1 - baseline) * s)
      )   
    }
  )
)

add_summary_stats = function(l) {
  l$data$mean = l$data$alpha / (l$data$alpha + l$data$beta)
  l$data$sd = sqrt(
    l$data$alpha * l$data$beta / (
      (l$data$alpha + l$data$beta)^2 * (l$data$alpha + l$data$beta + 1)
    )
  )
  l$data$tau = 1 / (l$data$sd  ^ 2)
  l
}

priors |> lapply(add_summary_stats) |> as_tibble(.name_repair = "minimal") |> t() |> 
  as_tibble() |> unnest(V1) |> rename(Prior = V1) |>
  unnest_wider(V2) |> select(-tau) |>
  kbl(caption = 'Priors and their parameters') |> kable_styling()
```

```{r}

geoms = lapply(
  priors, function(p) {
    geom_function(
      fun = dbeta,
      args = list(shape1 = p$data$alpha, shape2=p$data$beta),
      aes(color = p$name)
    ) 
  }
)  

prior_pdfs = ggplot() + geoms
```

```{r}

prior_pdfs + 
  labs(
    title='Conversion rate priors', 
    color='Prior', y='Density', x='Conversion rate'
  ) + scale_x_continuous(breaks=seq(0.11,0.23,0.02), limits=c(0.11, 0.23)) +
  theme_light() + theme(panel.grid=element_blank(), legend.position='bottom')

```

```{r}
geoms = lapply(
  priors, function(p) {
    geom_function(
      fun = pdf_beta_diff,
      args = list(p$data, p$data),
      aes(color = p$name, linetype='Exact difference')
    ) 
  }
)  

treatment_pdfs = ggplot() + geoms
```

If we take a zoomed-out view, it's clear that apart from the uninformed priors (which lead to a nearly (but not 100%!) flat treatment effect prior, all other choices tend to concentrate the prior probability of the treatment effect around zero.

```{r}
treatment_pdfs + 
  xlim(-1, 1) + 
  labs(
    title='Treatment priors - a zoomed out view', 
    color='Prior', y='Density', x='Treatment effect'
  ) +
  theme_light() + theme(panel.grid=element_blank(), legend.position='bottom') + 
  guides(linetype="none")
```

If we zoom in, the first thing that jumps out is that these shapes look very much Gaussian. That is not that surprising, given that Beta distributions converge to Gaussian and the difference of two independent Gaussians is a Gaussian. We can also clearly see what these priors imply - e.g., the strongest prior ("100% strength") suggests that a treatment effect larger than 0.02 is just not going to happen, while the weakest ("5% strength") prior allows for treatment effects up to 0.10.

```{r}
treatment_pdfs + xlim(-0.1, 0.1) +
  labs(
    title='Treatment priors - a zoomed in view', 
    color='Prior', y='Density', x='Treatment effect'
  ) +
  theme_light() + theme(panel.grid=element_blank(), legend.position='bottom') +
  guides(linetype="none")
```

```{r}

approx_normal_geoms = lapply(
  priors |> lapply(add_summary_stats), function(p) {
    geom_function(
      fun = dnorm,
      args = within(difference_of_two_normals(p$data, p$data), rm('tau')),
      aes(color = p$name, linetype='Normal approximation')
    ) 
  }
)

treatment_pdfs + approx_normal_geoms + 
  labs(
    title='Treatment priors - a zoomed out view', 
    color='Prior', y='Density', x='Treatment effect',
    linetype='Estimation approach'
  ) +
  theme_light() + theme(panel.grid=element_blank()) + 
  xlim(-0.1, 0.1) + scale_linetype_manual(values=c("dotdash", 'longdash'))
```

```{r}
priors |> lapply(add_summary_stats) |> lapply(function (p) {
  list(
    name=p$name,
    data = difference_of_two_normals(p$data, p$data)
  )
}) |> as_tibble(.name_repair = "minimal") |> t() |> 
  as_tibble() |> unnest(V1) |> rename(Prior = V1) |>
  unnest_wider(V2) |> select(-tau) |>
  mutate(
    `95% HDI` = paste0(
      '[', 
      format(mean + qnorm(0.025)*sd, digits=2), 
      ', ',
      format(mean + qnorm(0.975)*sd, digits=2), 
      ']'
    )
  ) |>
  kbl(caption = 'Implied Treatment Prior (Normal approximation)') |> kable_styling()
```

By now, if you actually work on experiments with conversion rates, I'm sure you have an opinion on which of these priors is most appropriate. You probably immediately think back to past X experiments you ran and the lifts you observed. That is the beauty of Bayesian statistics - you can use extra information. Objectively. Just like CUPED leverages pre-experiment metrics to gain more statistical power. No one ever has a problem with that.

We got here by using the "strength in relation to sample size" perspective, but, as I mentioned above, it's probably not the best way to reason about it. That's because your prior choice should not vary depending on expected sample size. It should be informed by your domain knowledge. Still, it's a useful vehicle that I want to keep on referring for a bit more.

Let's now look at the posteriors. Unsurprisingly, they are quite different.

```{r}

posteriors = lapply(
  priors, 
  function(p) {
    list(
      name=p$name,      
      data=list(
        B = calculate_beta_posterior(data$B, p$data),
        A = calculate_beta_posterior(data$A, p$data)
      )   
    )
  }
)   

treatment_posteriors = ggplot() + 
  lapply(
    posteriors, 
    function(p) {   
      geom_function(     
        fun = pdf_beta_diff,      
        args = list(       
          p$data$B,        
          p$data$A     
        ),     
        aes(color = p$name),   
      ) 
    }
  )      

treatment_posteriors + xlim(-0.02, 0.05) + 
  labs(
    title='Treatment posterior distributions', 
    color='Prior', y='Density', x='Treatment effect'
  ) +
  theme_light() + theme(panel.grid=element_blank(), legend.position='bottom')
```

It may be easier to compare them on a summary statistics basis.

```{r}
mean_diffs = sapply(
  posteriors, function (p) {   
    mean_B = p$data$B$alpha / (p$data$B$alpha + p$data$B$beta)   
    mean_A = p$data$A$alpha / (p$data$A$alpha + p$data$A$beta)   
    prob_above_zero = cdf_beta_diff(0, Inf, p$data$B, p$data$A)   
    list(prior=p$name, mean_diff=mean_B - mean_A, prob_above_zero = prob_above_zero)    
  })  

mean_diffs |> t() |> as_tibble() |> unnest(cols=everything()) |>    
  mutate(`Mean difference vs. flat prior estimate` = mean_diff / max(mean_diff)) |>
    kbl(caption='Summary statistics of treatment posteriors') |> kable_styling()
```

A couple of things jump out:

-   No matter what the prior is, we would make the same decision with all priors (assuming our threshold was 95% probability that treatment is positive).

-   The estimated posterior means for the treatment effect are quite different. The posterior mean is 2x smaller than the flat prior one. In fact, they correspond exactly to the strength of the priors (that's what the "factor" column illustrates).

## Accepting Bayesian estimates

![](bayes_treatment_effect.jpg){fig-align="center"}

## Statistical guarantees

```{r}


baseline = 0.17
n_group = 6000
sd_treatment_effect = 0.015
avg_treatment_effect = 0.00

eff_size_generator_normal = function()  {
  rnorm(1, avg_treatment_effect, sd_treatment_effect)
}

eff_size_density_normal = function(x) {
  dnorm(x, mean = avg_treatment_effect, sd = sd_treatment_effect) 
}

eff_size_generator_gamma = function() {
  rgamma(1, 2, 2) / 50 - 1/50 + avg_treatment_effect
}

eff_size_density_gamma = function(x) {
  dgamma((x - avg_treatment_effect + 1/50) * 50, 2, 2) * 50
}

eff_size_density = eff_size_density_normal
eff_size_generator = eff_size_generator_normal

mean(sapply(1:10000, function(i) eff_size_generator()))
sd(sapply(1:10000, function(i) eff_size_generator()))


```

```{r}

ggplot() + stat_function(fun = eff_size_density) + xlim(-0.05, 0.05)
```

```{r}

set.seed(42)
sim_results = pblapply(1:10000, function(i) {
  eff_size = eff_size_generator()
  baseline_observed = rbeta(1, n_group * baseline, n_group * (1 - baseline))
  A = rbinom(n_group, 1, baseline_observed)
  B = rbinom(n_group, 1, baseline_observed + eff_size)

  list(
    obs_baseline= baseline_observed,
    effect_size = eff_size,
    A_conversions = sum(A),
    B_conversions = sum(B),
    A_rate = mean(A),
    B_rate = mean(B),
    A_var = var(A),
    B_var = var(B),
    group_size = n_group,
    experiment = i
  )
  
}, cl=4)
```

```{r}
power.prop.test(n=n_group, p1=baseline, p2=baseline + avg_treatment_effect)
```

```{r, warning=FALSE, message=F}

integrate(function(x) {
  eff_size_density(x) * 
  replace_na(power.prop.test(n=n_group, p1=baseline, p2=baseline + x)$power, 0)
  
}, -Inf, Inf)
```

```{r}
sim_results |> bind_rows() |> ggplot() + geom_density(aes(
  x=effect_size, color='Simulated effect sizes')) + 
  stat_function(
    fun = eff_size_density,
    aes(color = 'Dist. overlay')
)
```

```{r}

fr_results = sim_results |> bind_rows() |>
  mutate(mean_diff = B_rate - A_rate) |> 
  mutate(sd_error = sqrt(B_var / group_size + A_var / group_size)) |>
  mutate(t_statistic = mean_diff / sd_error) |>
  mutate(is_stat_sig = abs(t_statistic) > qnorm(0.975)) |>
  mutate(S_error = is_stat_sig & (sign(mean_diff) != sign(effect_size))) |>
  mutate(correct = is_stat_sig & (sign(mean_diff) == sign(effect_size))) |>
  mutate(effect_size_above_mean = abs(effect_size) > avg_treatment_effect)
```

```{r}
fr_results |> summarize(
  not_stat_sig = 1-mean(is_stat_sig), 
  S_error_rate = mean(S_error), accuracy = mean(correct)
)
```

```{r}
fr_results |> 
  mutate(eff_size_bin = as.integer(round(effect_size, 3) * 1000)) |> 
  group_by(eff_size_bin) |>
  summarize(
    no_difference_detected = sum(1-is_stat_sig), 
    sign_error = sum(S_error), 
    accurate_detection = sum(correct)) |> 
  pivot_longer(-eff_size_bin) |>
  mutate(eff_size_bin = eff_size_bin / 1000) |>
  ggplot() + geom_col(aes(x=eff_size_bin, y=value, fill=name)) +
  labs(
    title='Accuracy of t-test as a function of true (unobservable) effect size',
    y = '# of experiments',
    x = 'Binned effect size',
    fill = 'Test outcome'
  ) +
  theme_light() + theme(panel.grid=element_blank(), legend.position='bottom') +
  scale_fill_manual(values=c('darkgreen', 'lightgrey', 'darkred'))
```

```{r}
fr_results |> 
  mutate(eff_size_bin = as.integer(round(effect_size, 3) * 1000)) |> 
  group_by(eff_size_bin) |>
  summarize(
    no_difference_detected = mean(1-is_stat_sig), 
    sign_error = mean(S_error), 
    accurate_detection = mean(correct)) |> 
  pivot_longer(-eff_size_bin) |>
  mutate(eff_size_bin = eff_size_bin / 1000) |>
  ggplot() + geom_col(aes(x=eff_size_bin, y=value, fill=name)) +
  labs(
    title='Accuracy of t-test as a function of true (unobservable) effect size',
    y = '# of experiments',
    x = 'Binned effect size',
    fill = 'Test outcome',
    linetype=''
  ) +
  theme_light() + theme(panel.grid=element_blank(), legend.position='right') +
  scale_fill_manual(values=c('darkgreen', 'lightgrey', 'darkred')) + 
  stat_function(
    fun = function(x) {
      1-power.prop.test(n=n_group, p1=baseline, p2=baseline+x)$power
    },
    aes(linetype='beta error rate (1 - statistical power)'),
    color='red'
  )
```

```{r}

evaluate_bayes = function(raw_results) {

  pblapply(priors, function(prior) {
    raw_results |> lapply(function(x) {
      A_posterior = list(data = list(
        alpha = prior$data$alpha + x$A_conversions,
        beta = prior$data$beta + (x$group_size - x$A_conversions)
      )) |> add_summary_stats()
      
      B_posterior = list(data = list(
          alpha = prior$data$alpha + x$B_conversions,
          beta = prior$data$beta + (x$group_size - x$B_conversions)
      )) |> add_summary_stats()
      
      approx_tr = difference_of_two_normals(B_posterior$data, A_posterior$data)
      
      x$prob_above_zero = 1 - pnorm(0, mean = approx_tr$mean, sd = approx_tr$sd)
      #x$prob_above_zero = cdf_beta_diff(0, Inf, B_posterior, A_posterior)
      x$prior = prior$name
      x
    })
  }, cl=4)

}

bayes_results = evaluate_bayes(sim_results)
```

```{r}
bayes_results |> bind_rows() |>
  mutate(mean_diff = B_rate - A_rate) |> 
  mutate(is_stat_sig = !between(prob_above_zero, 0.025, 0.975)) |>
  mutate(S_error = is_stat_sig & (sign(mean_diff) != sign(effect_size))) |>
  mutate(correct = is_stat_sig & (sign(mean_diff) == sign(effect_size))) |>
  group_by(prior) |>
  summarize(
  not_stat_sig = 1-mean(is_stat_sig), 
  S_error_rate = mean(S_error), accuracy = mean(correct)
)
```

```{r}
set.seed(42)
peek_size = 300
number_of_experiments = 5000
seq_sim_results = pblapply(1:number_of_experiments, function(j) {
  eff_size = eff_size_generator()
  baseline_observed = rbeta(1, n_group * baseline, n_group * (1 - baseline))
  A = rbinom(n_group, 1, baseline_observed)
  B = rbinom(n_group, 1, baseline_observed + eff_size)
  
  lapply(seq(peek_size, n_group, peek_size), function(i) {
    list(
      obs_baseline= baseline_observed,
      effect_size = eff_size,
      A_conversions = sum(A[1:i]),
      B_conversions = sum(B[1:i]),
      A_rate = mean(A[1:i]),
      B_rate = mean(B[1:i]),
      A_var = var(A[1:i]),
      B_var = var(B[1:i]),
      sequence = i/peek_size,
      group_size = i,
      experiment = j
    )
    
  })

  
  
}, cl=4)  |> unlist(recursive = F)
```

```{r}
bayes_seq_results = evaluate_bayes(seq_sim_results)
```

```{r}
bayes_summary_seq = bayes_seq_results |> bind_rows() |>
  group_by(prior, experiment) |>
  mutate(
    is_stat_sig_seq = !between(prob_above_zero, 0.025, 0.975)
  ) |>
  summarize(
    mean_diff = mean(B_rate) - mean(A_rate),
    effect_size = mean(effect_size),
    is_stat_sig = max(is_stat_sig_seq), 
    time_to_decision = min(if_else(is_stat_sig_seq, group_size, n_group))
    ) |>
  mutate(S_error = is_stat_sig & (sign(mean_diff) != sign(effect_size))) |>
  mutate(correct = is_stat_sig & (sign(mean_diff) == sign(effect_size))) 

bayes_summary_seq |>
  group_by(prior) |>
  summarize(
  not_stat_sig = 1-mean(is_stat_sig), 
  S_error_rate = mean(S_error), accuracy = mean(correct),
  t = mean(time_to_decision),
  tm = median(time_to_decision)
) |> kbl() |> kable_styling()
```

```{r}
bayes_summary_seq |> 
  mutate(eff_size_bin = as.integer(round(effect_size, 3) * 1000)) |> 
  group_by(eff_size_bin, prior) |>
  summarize(
    no_difference_detected = mean(1-is_stat_sig), 
    sign_error = mean(S_error), 
    accurate_detection = mean(correct)) |> 
  pivot_longer(-c(eff_size_bin, prior)) |>
  mutate(eff_size_bin = eff_size_bin / 1000) |>
  ggplot() + geom_col(aes(x=eff_size_bin, y=value, fill=name)) +
  labs(
    title='Accuracy of t-test as a function of true (unobservable) effect size',
    y = '# of experiments',
    x = 'Binned effect size',
    fill = 'Test outcome'
  ) +
  theme_light() + theme(panel.grid=element_blank(), legend.position='bottom') +
  scale_fill_manual(values=c('darkgreen', 'lightgrey', 'darkred')) + 
  facet_wrap(~prior) +
  stat_function(
    fun = function(x) {
      1-power.prop.test(n=n_group, p1=baseline, p2=baseline+x)$power
    },
    aes(linetype='beta error rate (1 - statistical power)'),
    color='red'
  )
```
