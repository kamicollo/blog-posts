---
title: "Statistical guarantees and long-term error rates in A/B testing"
subtitle: "Why considering just Type I and II error rates is not sufficient"
format: 
  html:
    code-fold: true
    code-summary: "Show code"
editor: visual
---

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
library(rbenchmark)
library(tidyverse)
library(ggplot2)
library(kableExtra)
library(pbapply)
```

Suppose you're the *Head of Something Data Related* in a company that runs many experiments annually, and your job is to ensure that the experimentation program is trustworthy. It's still in the early stages of the experimentation journey in this company, so most tests use null hypothesis testing (NHT) with fixed sample sizes (although you're looking into sequential testing and CUPED, too). You set the following rules for every experiment:

-   Only experiments with a p-value \< 0.05 will be considered as having conclusive outcomes (you target a Type I error rate $\alpha=0.05$).

-   Every experiment should be run with a predetermined sample size, which implies a power of 80% (you target a Type II error rate $\beta=0.1$) for the selected minimum detectable effect (MDE).

Setting aside the considerations on whether these are the [best thresholds and other issues with using NHT](https://aurimas.eu/blog/2023/02/getting-faster-to-decisions-in-a-b-tests-part-2-misinterpretations-and-practical-challenges-of-classical-hypothesis-testing/), here's one question that, arguably, should matter to someone responsible for an experimentation program:

> What long-term error rates (a.k.a. bad decisions) can I expect if I run the experimentation program with such parameters?

It may seem the answer is Type I and Type II error rates, or, "We will detect (i.e., declare them as statistically significant) differences when they exist, 80% of the time, and, in 5% of the cases we detect differences, they will be due to imperfect sampling randomization and not true differences". But that is only true under a few conditions:

-   You will have some experiments where the actual difference is precisely zero—not "close enough to zero," but literally zero.

-   When estimating the required sample size in every experiment, you will always choose MDE precisely equal to the actual unobservable difference for that specific experiment.

Neither of these conditions is likely to hold in the real world. You can get around the second assumption if the MDE used is "minimum effect you care about" - then, you can make a statement that "when the true effect is at least as large as the MDE, we will detect it with at least 80% of the time", but, in the world of online experiments, do you really not care about small effect sizes? A conversion rate improvement of 0.5 percentage point may still mean *many \$\$* annually, but detecting such an MDE requires 3-4x sample size (depending on the baseline) than an MDE of 1 percentage point. Are you willing to make that trade-off?

I'd argue that, in practice, practitioners tend to set MDEs above the minimum effects they care about. On top, conceptually, every experiment likely has a non-zero impact (they may be small, but not exactly zero - otherwise it may be worth disbanding the product team..). As a result, Type I and II error rates aren't likely to represent the long-term error rates. They have not been designed for that. Instead, they represent frequentist statistics and its definition of probability—the likelihood of observing the same results if you run the same experiment repeatedly.

As a *Head of Something Data,* however, you're not only interested in whether the results of a particular experiment hold. You (should!) want to know the long-term error rates.

For that, we need some other measures & some other tools.

## Moving beyond Type I error rates: Type M and S errors

Type I error rate, $\alpha$ is the most known statistical concept among everyone, including non-technical stakeholders. It's probably the most misinterpreted one, too, given all the issues people have with p-values.

For something so popular, I'd argue it's not so valuable because:

-   It's independent of the sample size. Isn't that weird? No matter how much data you collect, you're guaranteed that, if there's no underlying difference, 5% of the time, you will still find statistically significant differences in the data when using a p-value of 0.05. Intuitively, one would think that more data equals more confidence in results, but the Type I error rate doesn't change! Yes, the estimates become tighter, but the Type I error rate doesn't measure that.

-   How often do you run experiments that have precisely no impact? Hopefully, never. So you'll never need to worry about it! It's tempting to approximate zero with "effect sizes so small they don't matter," but Type I error rates say nothing about small effect sizes.

[Gelman and Carlin](https://journals.sagepub.com/doi/10.1177/1745691614551642) (2014) introduced two other metrics that I think are much more useful to consider:

-   **Type S (sign) error.** It's the probability that an observed difference, when declared statistically significant, has an opposite sign than the actual difference. In other words, it is the probability that you will make a decision opposite to what you are after.
-   **Type M (Magnitude) error** measures how much the observed difference, when declared statistically significant, differs from the actual difference, expressed as a ratio. It partially addresses the issue of quantifying the risk of declaring "effect sizes so small that they don't practically matter" as statistically significant and is closely related to the [winner's curse phenomenon](https://www.etsy.com/codeascraft/mitigating-the-winners-curse-in-online-experiments?utm_source=pocket_reader).

Unlike Type I errors, type M and S error rates directly translate to business implications. Furthermore, they become smaller as sample sizes increase, as they operate in the "when the alternative hypothesis is true" land, i.e., when the effect sizes are not exactly zero.

### Calculating Type S and M error rates

The original paper proposed calculating Type S and M error rates via simulation using the following function, where $A$ is the effect size, $s$ is the standard error, $\alpha$ is the chosen level of statistical significance, and $df$ represents the degrees of freedom.

```{r}
retrodesign <- function(A, s, alpha=.05, df=Inf, n.sims=10000){
  z <- qt(1-alpha/2, df)
  p.hi <- 1 - pt(z-A/s, df)
  p.lo <- pt(-z-A/s, df)
  power <- p.hi + p.lo
  typeS <- p.lo/power
  estimate <- A + s*rt(n.sims,df)
  significant <- abs(estimate) > s*z
  exaggeration <- mean(abs(estimate)[significant])/A
  return(list(power=power, typeS=typeS, typeM=exaggeration))
}
```

If we wanted to compute the metrics for a hypothetical A/B test with a binary outcome metric (e.g., conversion rate) where the baseline is *17%,* MDE of interest is 1 percentage point, and the sample size was 6000 users in each group:

```{r}
n_group = 6000
baseline = 0.17
delta = 0.01

#calculate the baseline variance
baseline_var = baseline * (1 - baseline)

#calculate standard error by estimating pooled variance
st_error = sqrt(baseline_var / n_group + baseline_var / n_group)

#degrees of freedom is N*2 - 2
df_test = n_group * 2 - 2

#use Gelman's function to get error rate estimates
gelman_formula = retrodesign(A=delta, s=st_error, df = df_test) |>
  as_tibble() |> mutate(method = 'Gelman/Carlin simulation')
  
gelman_formula |> kbl(caption = 'Estimated Metrics') |> kable_styling()
```

We can validate power calculations using R's built-in function. Power estimates are very close (differences due to simulation error).

```{r}
power.prop.test(n=n_group, p1=baseline, p2 = baseline + delta)
```

In 2019, [Lu, Qui, and Deng](https://pubmed.ncbi.nlm.nih.gov/29569719/) published a paper that includes closed-form formulas for these error rates. The implementation of them is straightforward, too (and can be easily extended to unequal sample size designs):

```{r}

calculate_rates = function(delta, sd, n) {
  # delta - effect size (MDE)
  # sd - baseline standard deviation
  # n - size per group
  lambda = abs(delta) / sqrt(sd**2/n + sd**2/n)
  z = qnorm(1 - 0.05/2)

  neg = pnorm(-z -lambda)
  diff = pnorm(z - lambda)
  pos = pnorm(z + lambda)
  inv_diff = pnorm(lambda - z)

  
  list(
    power = neg + 1 - diff,
    typeS = neg / (neg + 1 - diff),
    typeM = (
      dnorm(lambda + z) + dnorm(lambda - z) + lambda * (pos + inv_diff - 1)
    ) / (lambda * (1 - pos + inv_diff))
  )
  
}
```

We can verify that we get the same results:

```{r}

closed_form = calculate_rates(delta=delta, sd=sqrt(baseline_var), n = n_group) |>
  as_tibble() |> mutate(method = 'Closed-form formula')

bind_rows(gelman_formula, closed_form) |> 
  kbl(caption = 'Estimated Metrics') |> kable_styling()
```

Finally, we can verify that all these formulas are correct with a small simulation. In this simulation, we repeat the same experiment with the above parameters many times and estimate empirical error rate estimates.

```{r, message=FALSE, warning=F}

pboptions(type="none")
sim_results = pbsapply(1:10000, function(i) {
  set.seed(i*42)
  A = rbinom(n_group, 1, baseline)
  B = rbinom(n_group, 1, baseline + delta)
  mean_diff = mean(B) - mean(A)
  p_value = t.test(B, A)$p.value
  
  c(
    detected = p_value <= 0.05,
    sign_error = ifelse(p_value <= 0.05, sign(mean_diff) != sign(delta), NA),
    ratio = ifelse(p_value <= 0.05, mean_diff / delta, NA)
  )
  
}, cl=4) |> t() |> as_tibble()


empirical_rates = sim_results |> summarize(
  power = mean(detected, na.rm=T),
  typeS = mean(sign_error, na.rm=T),
  typeM = mean(ratio, na.rm=T),
  method = 'Empirical results'
)

bind_rows(gelman_formula, closed_form, empirical_rates) |> 
  kbl(caption = 'Estimated Metrics') |> kable_styling()

```

## Quantifying decision-making risk with Type S and M error rates in a single experiment

Lu, Qiu, and Deng's paper includes a few charts that I replicate below as they are excellent illustrations of the risk associated with Type S and M error rates. On the x-axis, we plot statistical power, and on the y-axis - Type S error rate / Type M ratio:

```{r}

error_rates = lapply(seq(500, 40000, 500), function(n) {
  calculate_rates(delta, sqrt(baseline_var), n)
}) |> bind_rows() |> pivot_longer(-power)


ggplot(error_rates, aes(x=power)) + 
  geom_line(aes(y=value, color=name)) +
  scale_x_continuous(breaks=seq(0.05, 0.95, 0.1)) +
  facet_wrap(~name, scales='free_y') +
  theme_light() + 
  theme(
    panel.grid.major.x = element_blank(),
    legend.position = "none"
  )
```

We can see that Type S error (which is really bad, decision-making-wise!) largely disappears once we hit power of  0.35−0.4. On the other hand, the type M ratio only goes below 1.5x once we achieve the statistical power of $0.5$. If this does not illustrate the issues with underpowered experiments, I don't know what does!

## Estimating long-term error rates

![](gohanks.jpg)

In my opinion, Type S and M error rates are much more helpful than pure Type I and II errors. However, just like Type II errors, they are not long-term error rates unless we assume we will perfectly set the MDE to the actual effect size in each experiment. If we could do that, we would not need to run experiments!

We could skirt around this issue by reframing error rates to focus on limits/margins, similar how it's done with MDE. However, a statement "we have at most *X%* risk of sign error when we detect statistically significant results and the underlying effect size is at least *Y*" feels not very satisfactory. Can we do better?

We can, but it requires an additional assumption (no free lunch, as always!). We can borrow an idea from Bayesian statistics and assume a distribution of likely effect sizes (a.k.a. prior distribution) instead of just working with fixed guesses.

Suppose we believe our average treatment effect is $0.015$. In a standard setting, we may choose to use it as an MDE. We could estimate the sample size required to achieve 80% power and a $17\%$ conversion baseline - that's about 10,200 observations per group.

```{r}
avg_treatment_effect = 0.015
required_sample_size = ceiling(power.prop.test(
  p1=baseline, 
  p2=baseline + avg_treatment_effect, 
  power=0.8
)$n)

required_sample_size
```

We can also estimate the corresponding Type S and M error rates given this particular treatment effect and sample size:

```{r}
calculate_rates(
  delta = avg_treatment_effect, 
  sd = sqrt(baseline * (1 - baseline)),
  n = required_sample_size
) |> as_tibble() |> 
  pivot_longer(everything(), names_to='Metric') |> 
  kbl(caption='Error rates at aa fixed effect size') |> kable_styling()
```

To get to long-term error rates (as well as the expectation of error rates for a single experiment, if repeated infinitely), we could, for example, assume that our effect sizes are distributed as a Normal distribution with $\mu=0.015, \sigma=0.014$. Or, perhaps a bit more realistically, we may choose a non-symmetric distribution and, keeping the same mean/standard distribution, assume that many experiments have smaller effect sizes, but there's also a longer tail of positive effects and no situations where experiments do very badly (because of safeguards such as dogfooding, UXR, etc.).

```{r, warning=F, message=F}

sd_treatment_effect = 0.014

eff_size_distrs = list(
  list(
    name = 'Typical team',
    distr = 'N(0.015, 0.014)',
    generator = function() rnorm(1, avg_treatment_effect, sd_treatment_effect),
    density = function(x) dnorm(x, mean = avg_treatment_effect, sd = sd_treatment_effect) 
  ),
  list(
    name = 'Mediocre team',
    distr = 'N(0, 0.014)',
    generator = function() rnorm(1, 0, sd_treatment_effect),
    density = function(x) dnorm(x, mean = 0, sd = sd_treatment_effect) 
  ),
  list(
    name = 'Downside-protected team',
    distr = 'shifted/scaled Gamma(2,2))',
    generator = function() rgamma(1, 2, 2) / 50 - 1/50 + avg_treatment_effect,
    density = function(x) dgamma((x - avg_treatment_effect + 1/50) * 50, 2, 2) * 50
  ),
  list(
    name = 'Golden team',
    distr = 'shifted/scaled Gamma(3,2)',
    generator = function() rgamma(1, 3, 2) / 50 - 1/50 + avg_treatment_effect,
    density = function(x) dgamma((x - avg_treatment_effect + 1/50) * 50, 3, 2) * 50
  ) 
)

emp = lapply(
  eff_size_distrs,
  function(d) {
    r = list()
    r[[d$name]] = sapply(1:50000, function(i) d$generator())
    r
  }
) |> bind_cols() |> pivot_longer(everything(), names_to = "Assumed distribution")

ggplot(emp) + 
  geom_density(aes(x=value, color=`Assumed distribution`)) +
  sapply(
  eff_size_distrs, 
  function(d) stat_function(
    aes(color=d$name, fill=d$name), 
    fun = d$density,
    data = tibble(name = d$name, full_name = paste(d$name, '~', d$distr)),
    geom="area",
    alpha=0.5
  )
) + 
  xlim(-0.05, 0.1) + labs(
    y='Density',
    color='Assumed effect size distribution',
    x='Effect size',
    title='Some possible effect size assumptions'
  ) + 
  theme_light() + 
  theme(
    panel.grid.major = element_blank(),
    legend.position = "none"
  ) + facet_wrap(~full_name) +
  geom_vline(xintercept = 0, linetype = 'dotted')

```

```{r}

emp |> group_by(`Assumed distribution`) |>
  summarize(
    average_treatment_effect = mean(value),
    Q10 = quantile(value, 0.1),
    Q90 = quantile(value, 0.9)
  ) |> 
  arrange(-average_treatment_effect) |>
  kbl(caption = 'Summary statistics of assumed effect size distributions') |>
  kable_styling()

```

How do we calculate, say, power over a distribution of possible effect sizes? It's actually pretty simple - we integrate:

```{r, warning=FALSE, message=F}

estimate_power = function(baseline, required_sample_size, density_func) {

  integrate(function(x) {
    p = power.prop.test(
      n=required_sample_size, 
      p1=baseline, 
      p2=baseline + x
    )$power
    
    density_func(x) * replace_na(p, 0)
    
  }, -Inf, Inf)$value
  
}

tibble(
  `Assumed effect size distribution` = sapply(eff_size_distrs, function(x) x$name),
  `P(detect | effect size distribution, N, baseline)` = sapply(
    eff_size_distrs,
    function(x) estimate_power(baseline, required_sample_size, x$density)
  )
) |> kbl(caption='Average "power" in the long run') |> kable_styling()
```

Our "average statistical power" (i.e. the probability we will declare an observed difference as statistically significant given our assumptions about sample size, baseline metric value and effect size distribution) is $66\%$ under the normal/symmetric prior distribution assumption and $56\%$ if we believe the asymmetric assumption is more appropriate. Both of them are much lower than the $80\%$ that the classical MDE calculation yields - which is not unexpected, as the latter only considers effect size equal to the MDE, while prior distributions include a lot of density of smaller effect sizes.

```{r}
p = seq(0.05, 0.99, 0.01)

avg_p = pblapply(p, function(power) {
  n = ceiling(
    power.prop.test(
      p1=baseline, 
      p2=baseline + avg_treatment_effect, 
      power=power
    )$n
  )
  lapply(eff_size_distrs, function(d) {
    list(name = d$name, v = estimate_power(baseline, n, d$density), power=power)
  })
  
}, cl=4) |> bind_rows()

ggplot(avg_p) + 
  geom_line(aes(x= power, y=v, color=name)) +
  geom_abline(intercept = 0, slope=1, linetype='dotted') +
  theme_light() + 
  theme(
    panel.grid.major = element_blank(),
    legend.position = "right"
  ) + labs(
    x = 'Power @ MDE',
    y = 'Avg. expected detection rate',
    color = 'Assumed effect size distribution',
    title = 'Long-run effect detection rates'
  ) + ylim(0, 1) + xlim(0, 1)
```

We can do the same with Type S error and Type M error. Type S error is straightforward - we can see that, on average, the risk of sign error is $0.2\%-0.3\%$. That is much higher than what a fixed effect size calculation suggests, but, as a long-term error rate, that's still very low.

```{r, warning=F, message=F}

estimate_typeS = function(baseline, required_sample_size, density_func) {
  
  baseline_sd = sqrt(baseline * (1 - baseline))

  integrate(function(x) {
    r = calculate_rates(n=required_sample_size, delta=x, sd=baseline_sd)
    r$typeS * r$power * density_func(x)
    
  }, -Inf, Inf, stop.on.error = F)$value

}

p = seq(0.05, 0.99, 0.01)

avg_sign_error = pblapply(p, function(power) {
  n = ceiling(
    power.prop.test(
      p1=baseline, 
      p2=baseline + avg_treatment_effect, 
      power=power
    )$n
  )
  lapply(eff_size_distrs, function(d) {
    list(name = d$name, v = estimate_typeS(baseline, n, d$density), power=power)
  })
  
}, cl=4) |> bind_rows()

ggplot(avg_sign_error) + 
  geom_line(aes(x= power, y=v, color=name)) +
  theme_light() + 
  theme(
    panel.grid.major = element_blank(),
    legend.position = "right"
  ) + labs(
    x = 'Power @ MDE',
    y = 'Avg. Type S error rate',
    color = 'Assumed effect size distribution',
    title = 'Long-run sign error rates'
  )

```

```{r}

estimate_typeM = function(baseline, required_sample_size, density_func) {
  
  baseline_sd = sqrt(baseline * (1 - baseline))

  integrate(function(x) {
    r = calculate_rates(n=required_sample_size, delta=x, sd=baseline_sd)
    (r$typeM - 1) * r$power * density_func(x) * abs(x)
    
  }, -Inf, Inf, stop.on.error = F)$value

}

p = seq(0.05, 0.99, 0.01)

avg_misestimate = pblapply(p, function(power) {
  n = ceiling(
    power.prop.test(
      p1=baseline, 
      p2=baseline + avg_treatment_effect, 
      power=power
    )$n
  )
  lapply(eff_size_distrs, function(d) {
    list(name = d$name, v = estimate_typeM(baseline, n, d$density), power=power)
  })
  
}, cl=4) |> bind_rows()

ggplot(avg_misestimate) + 
  geom_line(aes(x= power, y=v, color=name)) +
  theme_light() + 
  theme(
    panel.grid.major = element_blank(),
    legend.position = "right"
  ) + labs(
    x = 'Power @ MDE',
    y = 'Avg. Type S error rate',
    color = 'Assumed effect size distribution',
    title = 'Long-run sign error rates'
  )


```
