---
title: "Statistical guarantees and long-term error rates in A/B testing"
subtitle: "Why considering just Type I and II error rates is not sufficient"
format: html
editor: visual
---

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
library(rbenchmark)
library(tidyverse)
library(ggplot2)
library(kableExtra)
library(pbapply)
```

Suppose you're the *Head of Something Data Related* in a company that runs many experiments annually, and your job is to ensure that the experimentation program is trustworthy. It's still in the early stages of the experimentation journey in this company, so most tests use null hypothesis testing (NHT) with fixed sample sizes (although you're looking into sequential testing and CUPED, too). You set the following rules for every experiment:

-   Only experiments with a p-value \< 0.05 will be considered as having conclusive outcomes (you target a Type I error rate $\alpha=0.05$).

-   Every experiment should be run with a predetermined sample size, which implies a power of 80% (you target a Type II error rate $\beta=0.1$).

Setting aside the considerations on whether these are the [best thresholds and other issues with using NHT](https://aurimas.eu/blog/2023/02/getting-faster-to-decisions-in-a-b-tests-part-2-misinterpretations-and-practical-challenges-of-classical-hypothesis-testing/), here's one question that, arguably, should matter to someone responsible for an experimentation program:

> What long-term error rates (a.k.a. bad decisions) can I expect if I run the experimentation program with such parameters?

It may seem the answer is Type I and Type II error rates, or, "We will detect (i.e., declare them as statistically significant) differences when they exist, 80% of the time, and, in 5% of the cases we detect differences, they will be due to imperfect sampling randomization and not true differences". But that is only true under a few conditions:

-   When estimating the required sample size in every experiment, you will always choose MDE precisely equal to the actual unobservable difference for that specific experiment.

-   You will have some experiments where the actual difference is precisely zero—not "close enough to zero," but literally zero.

Neither of these conditions is likely to hold in the real world, so Type I and II error rates aren't likely to represent the long-term error rates. They have not been designed for that. Instead, they represent frequentist statistics and its definition of probability—the likelihood of observing the same results if you run the same experiment repeatedly.

As a *Head of Something Data,* however, you're not only interested in whether the results of a particular experiment hold. You (should!) want to know the long-term error rates.

For that, we need some other measures & some other tools.

## Moving beyond Type I error rates: Type M and S errors

Type I error rate, $\alpha$ is the most known statistical concept among everyone, including non-technical stakeholders. It's probably the most misinterpreted one, too, given all the issues people have with p-values.

For something so popular, I'd argue it's not so valuable because:

-   It's independent of the sample size. Isn't that weird? No matter how much data you collect, you're guaranteed that, if there's no underlying difference, 5% of the time, you will still find statistically significant differences in the data when using a p-value of 0.05. Intuitively, one would think that more data equals more confidence in results, but the Type I error rate doesn't change! Yes, the estimates become tighter, but the Type I error rate doesn't measure that.

-   How often do you run experiments that have precisely no impact? Hopefully, never. So you'll never need to worry about it! It's tempting to approximate zero with "effect sizes so small they don't matter," but Type I error rates say nothing about small effect sizes.

[Gelman and Carlin](https://journals.sagepub.com/doi/10.1177/1745691614551642) (2014) introduced two other metrics that I think are much more useful to consider:

-   **Type S (sign) error.** It's the probability that an observed difference, when declared statistically significant, has an opposite sign than the actual difference. In other words, it is the probability that you will make a decision opposite to what you are after.
-   **Type M (Magnitude) error** measures how much the observed difference, when declared statistically significant, differs from the actual difference, expressed as a ratio. It partially addresses the issue of quantifying the risk of declaring "effect sizes so small that they don't practically matter" as statistically significant and is closely related to the [winner's curse phenomenon](https://www.etsy.com/codeascraft/mitigating-the-winners-curse-in-online-experiments?utm_source=pocket_reader).

Unlike Type I errors, type M and S error rates directly translate to business implications. Furthermore, they become smaller as sample sizes increase, as they operate in the "when the alternative hypothesis is true" land, i.e., when the effect sizes are not exactly zero.

### Calculating Type S and M error rates

The original paper proposed calculating Type S and M error rates via simulation using the following function, where $A$ is the effect size, $s$ is the standard error, $\alpha$ is the chosen level of statistical significance, and $df$ represents the degrees of freedom.

```{r}
retrodesign <- function(A, s, alpha=.05, df=Inf, n.sims=10000){
  z <- qt(1-alpha/2, df)
  p.hi <- 1 - pt(z-A/s, df)
  p.lo <- pt(-z-A/s, df)
  power <- p.hi + p.lo
  typeS <- p.lo/power
  estimate <- A + s*rt(n.sims,df)
  significant <- abs(estimate) > s*z
  exaggeration <- mean(abs(estimate)[significant])/A
  return(list(power=power, typeS=typeS, typeM=exaggeration))
}
```

If we wanted to compute the metrics for a hypothetical A/B test with a binary outcome metric (e.g., conversion rate) where the baseline is *17%,* MDE of interest is *1ppt*, and the sample size was 6000 users in each group:

```{r}
n_group = 6000
baseline = 0.17
delta = 0.01

#calculate the baseline variance
baseline_var = baseline * (1 - baseline)

#calculate standard error by estimating pooled variance
st_error = sqrt(baseline_var / n_group + baseline_var / n_group)

#degrees of freedom is N*2 - 2
df_test = n_group * 2 - 2

#use Gelman's function to get error rate estimates
gelman_formula = retrodesign(A=delta, s=st_error, df = df_test) |>
  as_tibble() |> mutate(method = 'Gelman/Carlin simulation')
  
gelman_formula |> kbl(caption = 'Estimated Metrics') |> kable_styling()
```

We can validate power calculations using R's built-in function. Power estimates are very close (differences due to simulation error).

```{r}
power.prop.test(n=n_group, p1=baseline, p2 = baseline + delta)
```

In 2019, [Lu, Qui, and Deng](https://pubmed.ncbi.nlm.nih.gov/29569719/) published a paper that includes closed-form formulas for these error rates. The implementation of them is straightforward, too (and can be easily extended to unequal sample size designs):

```{r}

calculate_rates = function(delta, sd, n) {
  # delta - effect size (MDE)
  # sd - baseline standard deviation
  # n - size per group
  lambda = abs(delta) / sqrt(sd**2/n + sd**2/n)
  z = qnorm(1 - 0.05/2)

  neg = pnorm(-z -lambda)
  diff = pnorm(z - lambda)
  pos = pnorm(z + lambda)
  inv_diff = pnorm(lambda - z)

  
  list(
    power = neg + 1 - diff,
    typeS = neg / (neg + 1 - diff),
    typeM = (
      dnorm(lambda + z) + dnorm(lambda - z) + lambda * (pos + inv_diff - 1)
    ) / (lambda * (1 - pos + inv_diff))
  )
  
}
```

We can verify that we get the same results:

```{r}

closed_form = calculate_rates(delta=delta, sd=sqrt(baseline_var), n = n_group) |>
  as_tibble() |> mutate(method = 'Closed-form formula')

bind_rows(gelman_formula, closed_form) |> 
  kbl(caption = 'Estimated Metrics') |> kable_styling()
```

Finally, we can verify that all these formulas are correct with a small simulation. In this simulation, we repeat the same experiment with the above parameters many times and estimate empirical error rate estimates.

```{r, message=FALSE, warning=F}

pboptions(type="none")
sim_results = pbsapply(1:10000, function(i) {
  set.seed(i*42)
  A = rbinom(n_group, 1, baseline)
  B = rbinom(n_group, 1, baseline + delta)
  mean_diff = mean(B) - mean(A)
  p_value = t.test(B, A)$p.value
  
  c(
    detected = p_value <= 0.05,
    sign_error = ifelse(p_value <= 0.05, sign(mean_diff) != sign(delta), NA),
    ratio = ifelse(p_value <= 0.05, mean_diff / delta, NA)
  )
  
}, cl=4) |> t() |> as_tibble()


empirical_rates = sim_results |> summarize(
  power = mean(detected, na.rm=T),
  typeS = mean(sign_error, na.rm=T),
  typeM = mean(ratio, na.rm=T),
  method = 'Empirical results'
)

bind_rows(gelman_formula, closed_form, empirical_rates) |> 
  kbl(caption = 'Estimated Metrics') |> kable_styling()

```

## Quantifying decision-making risk with Type S and M error rates in a single experiment

Lu, Qiu, and Deng's paper includes a few charts that I replicate below as they are excellent illustrations of the risk associated with Type S and M error rates. On the x-axis, we plot statistical power, and on the y-axis - Type S error rate / Type M ratio:

```{r}

error_rates = lapply(seq(500, 40000, 500), function(n) {
  calculate_rates(delta, sqrt(baseline_var), n)
}) |> bind_rows() |> pivot_longer(-power)


ggplot(error_rates, aes(x=power)) + 
  geom_line(aes(y=value, color=name)) +
  scale_x_continuous(breaks=seq(0.05, 0.95, 0.1)) +
  facet_wrap(~name, scales='free_y') +
  theme_light() + 
  theme(
    panel.grid.major.x = element_blank(),
    legend.position = "none"
  )
```

We can see that Type S error (which is really bad, decision-making-wise!) largely disappears once we hit power of  0.35−0.4. On the other hand, the type M ratio only goes below 1.5x once we achieve the statistical power of $0.5$. If this does not illustrate the issues with underpowered experiments, I don't know what does!

## Estimating long-term error rates

In my opinion, Type S and M error rates are much more helpful than pure Type I and II errors. However, they are not long-term error rates unless we assume we will perfectly set the MDE to the actual effect size in each experiment. If we could do that, we would not need to run experiments!

Suppose we believe our average treatment effect is $0.015$. In a standard setting, we may choose to use it as an MDE. We could estimate the sample size required to achieve 80% power and a $17%$ conversion baseline:

```{r}
avg_treatment_effect = 0.015
required_sample_size = ceiling(power.prop.test(
  p1=baseline, 
  p2=baseline + avg_treatment_effect, 
  power=0.8
)$n)

required_sample_size
```

```{r}
calculate_rates(
  delta = avg_treatment_effect, 
  sd = sqrt(baseline * (1 - baseline)),
  n = required_sample_size
)
```
How do we get to long-term error rates? We borrow an idea from Bayesian statistics and assume a distribution of likely effect sizes (a.k.a. prior distribution).

![](gohanks.jpg)

Instead of setting an MDE of $0.015$, we could, for example, assume that our effect sizes are distributed as a Normal distribution with $\mu=0.015, \sigma=0.014$. Or, perhaps a bit more realistically, we may choose a non-symetric distribution and, keeping the same mean/standard distribution, assume that many experiments have smaller effect sizes, but there's also a longer tail of positive effects and no situations where experiments do very badly (because dogfooding, UXR, etc.).

```{r}

sd_treatment_effect = 0.014

eff_size_generator_normal = function()  {
  rnorm(1, avg_treatment_effect, sd_treatment_effect)
}

eff_size_density_normal = function(x) {
  dnorm(x, mean = avg_treatment_effect, sd = sd_treatment_effect) 
}

eff_size_generator_gamma = function() {
  rgamma(1, 2, 2) / 50 - 1/50 + avg_treatment_effect
}

eff_size_density_gamma = function(x) {
  dgamma((x - avg_treatment_effect + 1/50) * 50, 2, 2) * 50
}

ggplot() + 
  stat_function(
    aes(color='Normally distributed effect sizes'), 
    fun = eff_size_density_normal
  ) + 
  stat_function(
    aes(color='Asymmetrically distributed effect sizes'), 
    fun = eff_size_density_gamma
  ) + 
  xlim(-0.05, 0.1) + labs(
    y='Density',
    color='Assumed effect size distribution',
    x='Effect size',
    title='Some possible effect size assumptions'
  )

```
How do we calculate, say, power over a distribution of possible effect sizes? It's actually pretty simple - we integrate:

```{r, warning=FALSE, message=F}

estimate_power = function(baseline, required_sample_size, density_func) {

  integrate(function(x) {
    p = power.prop.test(
      n=required_sample_size, 
      p1=baseline, 
      p2=baseline + x
    )$power
    
    density_func(x) * replace_na(p, 0)
    
  }, -Inf, Inf)
  
}

estimate_power(baseline, required_sample_size, eff_size_density_gamma)

```


```{r, warning=FALSE, message=F}

baseline_sd = sqrt(baseline * (1 - baseline))

integrate(function(x) {
  r = calculate_rates(n=n_group, delta=x, sd=baseline_sd)
  eff_size_density(x) * (r$m_ratio - 1) * abs(x) * r$power

}, -Inf, Inf, stop.on.error = F)


integrate(function(x) {
  r = calculate_rates(n=n_group, delta=x, sd=baseline_sd)
  r$sign_error * r$power * eff_size_density(x)
  
}, -Inf, Inf, stop.on.error = F)

```

```{r}
ggplot() + stat_function(
  fun = function(x) { 
    r = retrodesign(
      A=x, s=sqrt(sd_baseline**2/n_group + sd_baseline**2/n_group), 
      df=n_group*2-2)
    abs(r$exaggeration)
}
) + xlim(-0.05, 0.05)
```
